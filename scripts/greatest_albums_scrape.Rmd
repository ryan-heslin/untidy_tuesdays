
# Scraping the Data

```{r, echo = FALSE}
url <- "https://www.rollingstone.com/music/music-lists/500-greatest-albums-of-all-time-156826/"
```

Today I'll attempt to classify album genres using text analysis. The data
come from [Rolling Stone's 2003 list](`r url`) of the "500 Greatest Albums of
All time." The magazine released an updated list last year, possibly in response to persistent criticisms that it overrepresented male-dominated
classic rock.

I don't place too much stock in these kinds of rankings, but this one does provide
an interesting web scraping and modeling challenge.
```{r}
genres_raw <- read_csv("../data/albumlist.csv", locale = locale(encoding = "macroman"))
genres_raw %>% count(Album,sort = TRUE) %>% 
  filter(n >1)
```

It seems there are three Greatest Hits albums (not surprising) and a second
_Let It Be_ (unexpected). Luckily, year is enough to distinguish them.

```{r}
genres_raw %>% filter(Album %in% c("Greatest Hits", "Let It Be"))
```
```{r}
genres <- rename_with(genres_raw, str_to_lower)
```

acylam from the `discoRd` sever helped me configure the driver and suggested code for the page scraping.

To reproduce this analysis as is, run a remote firefox server in a docker
container mapped to port 4445 by pasting this into a terminal:

`docker run -d -p 4445:4444 selenium/standalone-firefox:2.53.1`

```{r, results = "hide"}
driver <- remoteDriver(remoteServerAddr = "localhost",
               port= 4445L,
               browserName="firefox")

driver$open()
```

I define some functions to simplify scraping.
```{r, cache = TRUE}
scrape_page <- function(driver,
                        url,
                        wait = 5L,
                        sel) {
  driver$navigate(url)
  
  Sys.sleep(wait) 
  html_raw <- driver$getPageSource()[[1]]
  cat("Scraping", url, "\n")
  html2text(html_raw, sel)
  
}

html2text <- function(html_obj, sel, type = "css") {

  html <- read_html(html_obj)
  purrr::map(sel, get_html_elements, html = html) 
}

get_html_elements <- function(html, sel) {
    html_text(html_elements(x = html, css = sel))
  }
```

```{r, cache = TRUE}

driver$navigate(url)
links <- driver$getPageSource()[[1]] %>%
  read_html() %>%
  html_elements(css = "#pmc-gallery-list-nav-bar-render a") %>%
  html_attr("href")
```

Some descriptions on a few pages don't have the vertical gallery class, requiring me
to manually add another selector. Resolving this one special case took as much
time as the rest of the script.

```{r, echo = FALSE}
sel <-
  c(artist_album = ".c-gallery-vertical-album__title",
    label_year = "#pmc-gallery-vertical p:nth-child(1) em",
    description = "#pmc-gallery-vertical p:nth-child(2)")

pages <- tibble(link = links, sel = list(sel))
lapply(c(1, 3, 5), function(x) pages$sel[[x]][["description"]] <<-  paste(":nth-child(43) p ,", sel[["description"]]))
pages$sel[[3]][["label_year"]] <- c(label_year = "#pmc-gallery-vertical p:nth-child(1) em:nth-child(1)")
```

The actual scraping. It takes a long time.

```{r, cache =TRUE}
text_raw <- mapply(function(link, sel) scrape_page(driver, link, wait = 2.5, sel = sel), pages$link, pages$sel, SIMPLIFY = FALSE)
```

```{r}
driver$close()
```
